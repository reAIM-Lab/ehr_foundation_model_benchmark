{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[[0.1,0.2],[0.5,0.6],[0.5,0.6],[0.5,0.6]],[[0.3,0.4],[0.5,0.8],[0.2,0.3],[0.4,0.5],]])\n",
    "print(a)\n",
    "# print(a.sum(dim=0).sum(dim=1))\n",
    "\n",
    "# b = torch.cumsum(a, dim=1)\n",
    "# print(b)\n",
    "# print(b[:,:-1])\n",
    "\n",
    "# c = torch.cat([torch.ones_like(a[:, :1]), 1.0 - b[:,:-1]], dim=1)\n",
    "\n",
    "cdf = torch.cumsum(a, dim=1)\n",
    "print(cdf)\n",
    "c = torch.cat(\n",
    "    [torch.ones_like(a[:, :1, :]),\n",
    "     1.0 - cdf[:, :-1, :]],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabb2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False,  True, False],\n",
      "        [False, False, False,  True],\n",
      "        [ True,  True,  True, False]])\n",
      "tensor([[False,  True, False],\n",
      "        [False, False, False],\n",
      "        [ True,  True,  True]])\n",
      "tensor([[[False, False, False],\n",
      "         [False, False, False],\n",
      "         [False,  True, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [ True,  True,  True],\n",
      "         [False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "## unit test for torch.where and torch.zeros_like\n",
    "import torch\n",
    "a1 = torch.rand((3,4))\n",
    "a2 = torch.rand((3,3))\n",
    "\n",
    "time= a1>0.5\n",
    "value = a2>0.7\n",
    "\n",
    "3,4,3\n",
    "\n",
    "event_bool = time.unsqueeze(2) & value.unsqueeze(1)\n",
    "\n",
    "print(time)\n",
    "print(value)\n",
    "print(event_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e75187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, Iterator, List, Mapping, Optional, Sequence, Set, Tuple\n",
    "import scipy.sparse\n",
    "\n",
    "def sparse_to_dense(sparse_data):\n",
    "    \"\"\"Convert sparse data to dense tensor\"\"\"\n",
    "    shape = (10,1000)\n",
    "    sparse_dict = {k: v for k, v in sparse_data.items()}\n",
    "    \n",
    "    # Handle the case where we have 'value' field in sparse data\n",
    "    if \"value\" in sparse_dict:\n",
    "        # For value sparse data, we need to return both time and value\n",
    "        time_sparse = scipy.sparse.csr_array(\n",
    "            (sparse_dict[\"time\"], sparse_dict[\"indices\"], sparse_dict[\"indptr\"]), \n",
    "            shape=shape\n",
    "        )\n",
    "        \n",
    "        # Initialize value tensor with NaN (None equivalent) instead of 0\n",
    "        # Values can range from -inf to +inf, so 0 is not appropriate default\n",
    "        value_dense = torch.full((10, 1000), float('nan'), dtype=torch.float32)\n",
    "        \n",
    "        # Fill in actual values only where they exist\n",
    "        for i in range(len(sparse_dict[\"indptr\"]) - 1):\n",
    "            start = sparse_dict[\"indptr\"][i]\n",
    "            end = sparse_dict[\"indptr\"][i + 1]\n",
    "            if end > start:\n",
    "                indices = sparse_dict[\"indices\"][start:end]\n",
    "                values = sparse_dict[\"value\"][start:end]\n",
    "                value_dense[i, indices] = torch.tensor(values, dtype=torch.float32)\n",
    "        \n",
    "        return torch.from_numpy(time_sparse.toarray()), value_dense\n",
    "    else:\n",
    "        # Regular time sparse data\n",
    "        sparse_matrix = scipy.sparse.csr_array(\n",
    "            (sparse_dict[\"time\"], sparse_dict[\"indices\"], sparse_dict[\"indptr\"]), \n",
    "            shape=shape\n",
    "        )\n",
    "        return torch.from_numpy(sparse_matrix.toarray())\n",
    "\n",
    "\n",
    "time_sparse: Dict[str, List[float]] = {\n",
    "    \"time\": [],\n",
    "    \"indices\": [],\n",
    "    \"indptr\": [0],\n",
    "}\n",
    "result = {\n",
    "        \"time\": np.array(time_sparse[\"time\"]),\n",
    "        \"indices\": np.array(time_sparse[\"indices\"]),\n",
    "        \"indptr\": np.array(time_sparse[\"indptr\"]),\n",
    "    }\n",
    "\n",
    "time_non_numerical = sparse_to_dense(result)\n",
    "print(time_non_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb98c7",
   "metadata": {},
   "source": [
    "## LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib      \n",
    "from tqdm import tqdm\n",
    "import meds_reader\n",
    "import femr.stat_utils\n",
    "a = femr.stat_utils.ReservoirSampler(100000)\n",
    "meds_reader_path = \"/user/zj2398/cache/mimic/meds_v0.6_reader\"\n",
    "database = meds_reader.SubjectDatabase(meds_reader_path)\n",
    "lab_dict = {}\n",
    "numerical_values= []\n",
    "for subject_id in tqdm(database):\n",
    "    for event in database[subject_id].events:\n",
    "        if event.numeric_value is not None:\n",
    "            if event.code not in lab_dict:\n",
    "                lab_dict[event.code] = [1,femr.stat_utils.ReservoirSampler(100000)]\n",
    "                lab_dict[event.code][1].add(float(event.numeric_value),1)\n",
    "            else: \n",
    "                lab_dict[event.code][0] += 1\n",
    "                lab_dict[event.code][1].add(float(event.numeric_value),1)\n",
    "            # print(event.code,event.numeric_value,event.text_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53087047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.models.tokenizer\n",
    "import pickle\n",
    "import meds\n",
    "tokenizer_path = \"/user/zj2398/cache/motor_mimic_bin_20/tokenizer\"\n",
    "ontology_path = \"/user/zj2398/cache/motor_mimic_bin_20/ontology.pkl\"\n",
    "with open(ontology_path, 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "tokenizer = femr.models.tokenizer.HierarchicalTokenizer.from_pretrained(tokenizer_path, ontology=ontology)\n",
    "print(len(tokenizer.code_lookup))\n",
    "\n",
    "# subject count = 364627\n",
    "sorted_items = sorted(lab_dict.items(), key=lambda x: x[1][0],reverse=True)\n",
    "# print(sorted_items)\n",
    "# print(len(sorted_items))\n",
    "in_dict = {}\n",
    "out_dict = {}\n",
    "for key,value in sorted_items:\n",
    "    if key in tokenizer.code_lookup and value[0] > 3647:\n",
    "        in_dict[key] = value\n",
    "    else:\n",
    "        out_dict[key] = value\n",
    "\n",
    "print(len(in_dict))\n",
    "print(in_dict)\n",
    "print(len(out_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6798dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of each code in the database\n",
    "frequency_list = list(in_dict.keys())\n",
    "idx_dict = {code: i for i, code in enumerate(frequency_list)}\n",
    "# print(idx_dict)\n",
    "\n",
    "subject_counts = [0] * len(frequency_list)\n",
    "for subject_id in tqdm(database):\n",
    "    # track which codes appeared for this subject\n",
    "    seen_codes = set()\n",
    "    for event in database[subject_id].events:\n",
    "        if event.code in in_dict and event.numeric_value is not None:\n",
    "            seen_codes.add(event.code)\n",
    "    \n",
    "    # update subject-level counts\n",
    "    for code in seen_codes:\n",
    "        subject_counts[idx_dict[code]] += 1\n",
    "\n",
    "print(len(subject_counts))\n",
    "print([i/364627 for i in subject_counts])\n",
    "print(sum([0.999>(i/364627)>0.001 for i in subject_counts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# summary statistics\n",
    "s = pd.Series(count_list)\n",
    "summary = pd.DataFrame({\n",
    "    \"count\": [s.size],\n",
    "    \"unique\": [s.nunique()],\n",
    "    \"min\": [s.min()],\n",
    "    \"p1\": [s.quantile(0.01)],\n",
    "    \"p5\": [s.quantile(0.05)],\n",
    "    \"p25\": [s.quantile(0.25)],\n",
    "    \"median\": [s.median()],\n",
    "    \"p75\": [s.quantile(0.75)],\n",
    "    \"p95\": [s.quantile(0.95)],\n",
    "    \"p99\": [s.quantile(0.99)],\n",
    "    \"max\": [s.max()]\n",
    "})\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# log-spaced bins\n",
    "b = np.logspace(np.log10(min(count_list)), np.log10(max(count_list)), 60)\n",
    "plt.figure()\n",
    "plt.hist(count_list, bins=b)\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Value (log scale)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram with log-spaced bins\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.sort(count_list)\n",
    "y = np.arange(1, x.size + 1) / x.size\n",
    "plt.figure()\n",
    "plt.plot(x, y, drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d48e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbb467ce",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb91b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.models.tokenizer\n",
    "import pickle\n",
    "tokenizer_path = \"/user/zj2398/cache/motor_mimic/tokenizer\"\n",
    "ontology_path = \"/user/zj2398/cache/motor_mimic/ontology.pkl\"\n",
    "with open(ontology_path, 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "tokenizer = femr.models.tokenizer.HierarchicalTokenizer.from_pretrained(tokenizer_path, ontology=ontology)\n",
    "print(tokenizer.dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d307ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# store_path = \"/user/zj2398/cache/motor/tokenizer\"\n",
    "# with open(\"dictionary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(tokenizer.dictionary, f)\n",
    "# print(len(tokenizer.dictionary['vocab']))\n",
    "print(tokenizer.numeric_values)\n",
    "print(tokenizer.numeric_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b171006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from femr.models.tokenizer.hierarchical_tokenizer import HierarchicalTokenizer\n",
    "import pathlib\n",
    "import meds_reader\n",
    "meds_reader_path = \"/user/zj2398/cache/mimic/meds_v0.6_reader\"\n",
    "database = meds_reader.SubjectDatabase(meds_reader_path)\n",
    "\n",
    "pretraining_data_path = pathlib.Path(\"/user/zj2398/cache/deephit_tpp_8k\")\n",
    "task_path = pretraining_data_path / \"motor_task.pkl\"\n",
    "with open(task_path, 'rb') as f:\n",
    "    motor_task = pickle.load(f)\n",
    "task_info = motor_task.pretraining_task_info\n",
    "# list_task_info = list(i[0] for i in task_info)\n",
    "print(task_info)\n",
    "print([a[1] for a in motor_task.pretraining_task_info])\n",
    "# print(motor_task.task_to_index_map)\n",
    "# print(motor_task.numerical_task_to_index_map)\n",
    "\n",
    "# for i in motor_task.numerical_task_to_index_map.keys():\n",
    "#     a = i.split(\"//\")\n",
    "#     if len(a)< 2:\n",
    "#         print(i)\n",
    "# code_list = [i.split(\"//\")[1] for i in motor_task.numerical_task_to_index_map.keys()]\n",
    "# print(len(code_list))\n",
    "# print(code_list)\n",
    "# for i in code_list:\n",
    "#     try:\n",
    "#         int(i)\n",
    "#     except:\n",
    "#         print(i)\n",
    "# lab_test_list = [51265,50885,50912,50821,225170,220045,220210]\n",
    "\n",
    "# for i in lab_test_list:\n",
    "#     if i not in code_list:\n",
    "#         print(i)\n",
    "# print(code_list)\n",
    "'''\n",
    "\n",
    "Platelet Count → 51265 (complete blood count). \n",
    "PMC\n",
    "Bilirubin, Total → 50885 (chemistry). \n",
    "MIMIC\n",
    "PhysioNet\n",
    "Creatinine (serum/plasma) → 50912 (chemistry). \n",
    "GitLab\n",
    "PaO₂ (arterial blood gas “PO2”) → 50821 (blood gas).\n",
    "'''\n",
    "\n",
    "# ontology_path = pretraining_data_path / 'ontology.pkl'\n",
    "# tokenizer_path = pretraining_data_path / 'tokenizer'\n",
    "# with open(ontology_path, 'rb') as f:\n",
    "#     ontology = pickle.load(f)\n",
    "# tokenizer = HierarchicalTokenizer.from_pretrained(tokenizer_path, ontology=ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ec8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import meds_reader\n",
    "meds_reader_path = \"/user/zj2398/cache/mimic/meds_v0.6_reader\"\n",
    "database = meds_reader.SubjectDatabase(meds_reader_path)\n",
    "count = 0\n",
    "# type_list = []\n",
    "# numerical_value_list = ['LAB','SUBJECT_FLUID_OUTPUT','INFUSION_START','SUBJECT_WEIGHT_AT_INFUSION','INFUSION_END']\n",
    "numerical_code_list = []\n",
    "\n",
    "for subject_id in database:\n",
    "    subject = database[subject_id]\n",
    "    for event in subject.events:\n",
    "        if event.numeric_value is not None:\n",
    "            try:\n",
    "                float_value = float(event.numeric_value)\n",
    "                numerical_code_list.append(event.code)\n",
    "            except:\n",
    "                continue\n",
    "            # print(event.code,event.numeric_value,event.text_value)\n",
    "        # if :\n",
    "        #     event_type = event.code.split(\"//\")[0]\n",
    "        #     if event_type not in type_list:\n",
    "        #         type_list.append(event_type)\n",
    "        #         print(event_type)\n",
    "        #     # if count > 100:\n",
    "        #     #     sys.exit()\n",
    "        #     # print(event.code,event.numeric_value,event.text_value)\n",
    "        #     try:\n",
    "        #         # print(event.numeric_value)\n",
    "        #         float_value = float(event.numeric_value)\n",
    "        #         # if float_value < 0:\n",
    "        #         #     print(event.code,event.numeric_value,event.time,event.text_value)\n",
    "        #         #     count += 1\n",
    "        #     except:\n",
    "        #         continue\n",
    "                # print(f\"error {event.code} {event.numeric_value} {event.text_value}\")\n",
    "\n",
    "print(len(numerical_code_list))\n",
    "print(numerical_code_list)\n",
    "# event is always not None except for GENDER\n",
    "\n",
    "\n",
    "# for subject_id in database:\n",
    "#     subject = database[subject_id]\n",
    "#     for event in subject.events:\n",
    "#         if event.time is None and not event.code.startswith(\"GENDER\"):\n",
    "#             print(event.code,event.numeric_value,event.text_value)\n",
    "            # float_value = float(event.numeric_value)\n",
    "            # if float_value < 0:\n",
    "            #     print(event.code,event.numeric_value,event.time,event.text_value)\n",
    "            #     count += 1\n",
    "\n",
    "\n",
    "'''\n",
    "LAB\n",
    "SUBJECT_FLUID_OUTPUT\n",
    "INFUSION_START\n",
    "SUBJECT_WEIGHT_AT_INFUSION\n",
    "INFUSION_END\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3704181e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2],[3,4],[5,6]])\n",
    "b = a.tolist()\n",
    "print(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a919707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# print(len(numerical_code_list))\n",
    "# print(numerical_code_list)\n",
    "\n",
    "dict1 = {\"a\":1,\"b\":2}\n",
    "print(list(dict1.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.models.tokenizer\n",
    "print(tokenizer.string_lookup['unit'])\n",
    "print(len(tokenizer.string_lookup['unit'])) \n",
    "# ['route', 'frequency', 'text_value', 'language', 'race', 'priority', 'marital_status', 'insurance', 'unit', 'statusdescription', 'ordercategorydescription']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa093ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = list(tokenizer.code_lookup.keys())\n",
    "# # print(len(a))\n",
    "# print(a)\n",
    "print(tokenizer.code_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65cc900",
   "metadata": {},
   "source": [
    "### Motor_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "task_path = '/user/zj2398/cache/motor/motor_task.pkl'\n",
    "with open(task_path, 'rb') as f:\n",
    "    motor_task = pickle.load(f)\n",
    "print(motor_task.time_bins)\n",
    "a = motor_task.pretraining_task_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cf8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meds_reader_convert mimic_meds mimic_meds_reader --num_threads 32\n",
    "import os\n",
    "import pandas as pd\n",
    "base_path = \"/user/zj2398/cache/mimic/mimic-3.1-meds/patient_outcome_tasks/task\"\n",
    "for dir in os.listdir(base_path):\n",
    "    dir_path = os.path.join(base_path, dir)\n",
    "    if os.path.isdir(dir_path):\n",
    "        parquet_file = os.path.join(dir_path, f\"{dir}.parquet\")\n",
    "        if os.path.exists(parquet_file):\n",
    "        #     print(f\"The parquet file {parquet_file} exists\")\n",
    "        # else:\n",
    "            # df = pd.concat([df_train, df_tuning, df_test])\n",
    "            # df = df.sort_values(by=[\"subject_id\", \"prediction_time\"])\n",
    "            # df.to_parquet(parquet_file)\n",
    "            # df_train.to_parquet(os.path.join(dir_path, f\"{dir}.parquet\"))\n",
    "            # df_val.to_parquet(os.path.join(dir_path, f\"{dir}.parquet\"))\n",
    "            # df_test.to_parquet(os.path.join(dir_path, f\"{dir}.parquet\"))\n",
    "    # if os.path.isdir(os.path.join(base_path, dir)):\n",
    "    # print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dir_path = \"/user/zj2398/cache/mimic/mimic-3.1-meds/patient_outcome_tasks/task/long_los/tuning.parquet\"\n",
    "df = pd.read_parquet(dir_path)\n",
    "print(df)\n",
    "df2 = df.to_dict(orient=\"records\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "task_path = \"/user/zj2398/cache/motor_mimic/motor_task.pkl\"\n",
    "with open(task_path, 'rb') as f:\n",
    "    motor_task = pickle.load(f)\n",
    "# print(motor_task.pretraining_task_codes)\n",
    "code_dict = {}\n",
    "for i in motor_task.pretraining_task_codes:\n",
    "    code_type = i.split(\"//\")[0]\n",
    "    if code_type not in code_dict:\n",
    "        code_dict[code_type] = 1\n",
    "    else:\n",
    "        code_dict[code_type] += 1\n",
    "print(len(motor_task.pretraining_task_codes))\n",
    "for i in code_dict:\n",
    "    print(i, code_dict[i])\n",
    "\n",
    "# print(motor_task.time_bins)\n",
    "# print(motor_task.final_layer_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c55362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# a = torch.tensor([\n",
    "#   # pred point 0\n",
    "#   [[True, False, False],    # bin 0\n",
    "#    [False, False, False]],  # bin 1\n",
    "\n",
    "#   # pred point 1\n",
    "#   [[False, False, False],\n",
    "#    [False, False, True]]\n",
    "# ])\n",
    "# a = torch.tensor([[[0.4906, 0.0603, 0.3938, 0.1182],\n",
    "#          [0.8817, 0.2784, 0.2033, 0.4675],\n",
    "#          [0.0389, 0.5167, 0.3233, 0.8253]],\n",
    "\n",
    "#         [[0.5043, 0.1975, 0.6027, 0.8749],\n",
    "#          [0.4877, 0.4640, 0.9564, 0.4848],\n",
    "#          [0.4997, 0.4962, 0.2766, 0.7736]]])\n",
    "# a = torch.randint(0, 2, (2, 3, 4), dtype=torch.bool)\n",
    "b= torch.tensor([[[ True, False, False,  True],\n",
    "         [ True, False,  True,  True],\n",
    "         [ True, False,  True,  True]],\n",
    "\n",
    "        [[False,  True,  True,  True],\n",
    "         [ True,  True, False, False],\n",
    "         [False,  True, False,  True]]])\n",
    "c = torch.any(b, dim=1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([float(\"-inf\"),1,1,1,1,1,1,1,1,1,float(\"inf\")])\n",
    "\n",
    "def keep_unique_bins(value_samples, num_bins):\n",
    "    unique_bins = np.unique(value_samples)\n",
    "    unique_bins_augmented = np.concatenate([unique_bins,np.array((num_bins+1-len(unique_bins))*[float(\"inf\")])])\n",
    "    mask = np.concatenate([np.ones(len(unique_bins)),np.zeros(num_bins+1-len(unique_bins))])\n",
    "    return unique_bins_augmented, mask\n",
    "\n",
    "unique_bins, mask = keep_unique_bins(a, 10)\n",
    "print(unique_bins,unique_bins.dtype)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b3da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motor_chao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
