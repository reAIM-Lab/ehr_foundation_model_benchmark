{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a154d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_hospital_mortality\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "# def read_recursive_parquet(root_dir):\n",
    "#     all_files = glob.glob(os.path.join(root_dir, '**', '*.parquet'), recursive=True)\n",
    "#     print(all_files)\n",
    "#     df = pd.concat((pd.read_parquet(f) for f in all_files), ignore_index=True)\n",
    "#     return df\n",
    "\n",
    "cohort_dir = \"/data/processed_datasets/processed_datasets/mimic/mimic_3.1/patient_outcome_tasks/in_hospital_mortality\"\n",
    "# cohort = read_recursive_parquet(cohort_dir)\n",
    "# print(cohort.prediction_time)\n",
    "label_name = os.path.basename(os.path.normpath(cohort_dir))\n",
    "print(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d956e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422757\n",
      "422757\n",
      "1156492\n",
      "1156492\n",
      "3231218\n",
      "3231218\n",
      "3129231\n",
      "3129231\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3982bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214715 [10000032, 10000032, 10000032, 10000117, 10000690, 10000690, 10000690, 10000826, 10000826, 10000935, 10000935, 10000980, 10000980, 10000980, 10000980, 10000980, 10001186, 10001186, 10001338, 10001338, 10001338, 10001401, 10001401, 10001401, 10001401, 10001401, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10002013, 10002013, 10002013]\n",
      "480177 [10000032, 10000032, 10000032, 10000117, 10000690, 10000690, 10000690, 10000826, 10000826, 10000935, 10000935, 10000980, 10000980, 10000980, 10000980, 10000980, 10001186, 10001186, 10001338, 10001338, 10001338, 10001401, 10001401, 10001401, 10001401, 10001401, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10002013, 10002013, 10002013]\n",
      "209133 [10000635, 10000690, 10000690, 10000690, 10000935, 10000935, 10000935, 10000935, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10001186, 10001186, 10001217, 10001401, 10001401, 10001401, 10001401, 10001401, 10001401, 10001401, 10001401, 10001877, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10002013, 10002013, 10002013]\n",
      "405117 [10000635, 10000690, 10000690, 10000690, 10000935, 10000935, 10000935, 10000935, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10001186, 10001186, 10001217, 10001401, 10001401, 10001401, 10001401, 10001401, 10001401, 10001401, 10001401, 10001877, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10001884, 10002013, 10002013, 10002013]\n",
      "314140 [10000635, 10000635, 10000635, 10000635, 10000635, 10000635, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10001176, 10001186, 10001186, 10001843, 10001877, 10001877, 10002013, 10002013, 10002013, 10002013, 10002013, 10002013, 10002013]\n",
      "1438014 [10000635, 10000635, 10000635, 10000635, 10000635, 10000635, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000935, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10000980, 10001176, 10001186, 10001186, 10001843, 10001877, 10001877, 10002013, 10002013, 10002013, 10002013, 10002013, 10002013, 10002013]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import pandas as pd\n",
    "import pickle\n",
    "disease_dir = Path(\"/user/zj2398/cache/mtpp_8k/labels\")\n",
    "feature_dir = Path(\"/user/zj2398/cache/mtpp_8k/mtpp_mean_all/features\")\n",
    "# feature_dir = Path(\"/user/zj2398/cache/mtpp_8k/mtpp_L_num/features\")\n",
    "\n",
    "# for task in [\"celiac\",\"masld\",\"stroke\"]:\n",
    "for task in [\"celiac\"]:\n",
    "    disease_name = f\"{task}.parquet\"\n",
    "    disease_path = disease_dir /disease_name\n",
    "    disease_df = pd.read_parquet(disease_path)\n",
    "    # print(disease_df)\n",
    "    print(len(disease_df),sorted(list(disease_df[\"subject_id\"]))[:50])\n",
    "\n",
    "    feature_name = f\"{task}_motor.pkl\"\n",
    "    feature_path = feature_dir /feature_name\n",
    "    with open (feature_path,\"rb\") as f:\n",
    "        feature_df = pickle.load(f)\n",
    "    # print(len(feature_df[\"subject_ids\"]))\n",
    "    # print(len(feature_df[\"feature_times\"]))\n",
    "    list1= sorted(list(feature_df[\"subject_ids\"]))\n",
    "    print(len(list1),list1[:50])\n",
    "\n",
    "\n",
    "# feature_name = \"stroke_motor.pkl\"\n",
    "# feature_path = feature_dir /feature_name\n",
    "# with open (feature_path,\"rb\") as f:\n",
    "#     feature_df = pickle.load(f)\n",
    "# list1= sorted(list(feature_df[\"subject_ids\"]))\n",
    "# print(len(list1),list1[:50])\n",
    "\n",
    "# feature_name = \"masld_motor.pkl\"\n",
    "# feature_path = feature_dir /feature_name\n",
    "# with open (feature_path,\"rb\") as f:\n",
    "#     feature_df = pickle.load(f)\n",
    "# list1= sorted(list(feature_df[\"subject_ids\"]))\n",
    "# print(len(list1),list1[:50])\n",
    "\n",
    "\n",
    "# 465758\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8023663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        subject_id     prediction_time  boolean_value  censor_label  \\\n",
      "0         14395835 2130-05-18 05:50:00          False         False   \n",
      "1         14395835 2132-03-22 20:55:00          False         False   \n",
      "2         14395835 2135-08-28 14:34:00          False         False   \n",
      "3         14395835 2135-09-08 06:31:00          False         False   \n",
      "4         14396009 2140-03-12 20:34:00          False         False   \n",
      "...            ...                 ...            ...           ...   \n",
      "214710    19999784 2123-07-17 00:00:00          False         False   \n",
      "214711    19999784 2123-08-14 08:40:00          False         False   \n",
      "214712    19999784 2123-11-12 08:49:00          False         False   \n",
      "214713    19999784 2124-03-23 00:00:00          False         False   \n",
      "214714    19999828 2149-01-08 16:44:00          False         False   \n",
      "\n",
      "        time_to_event_days  \n",
      "0              2289.216667  \n",
      "1              1614.588194  \n",
      "2               360.852778  \n",
      "3               350.188194  \n",
      "4                91.143056  \n",
      "...                    ...  \n",
      "214710          401.979861  \n",
      "214711          373.618750  \n",
      "214712          283.612500  \n",
      "214713          151.979861  \n",
      "214714           10.636111  \n",
      "\n",
      "[214715 rows x 5 columns]\n",
      "overlapping features         subject_id     prediction_time  boolean_value  censor_label  \\\n",
      "0         14395835 2130-05-18 05:50:00          False         False   \n",
      "1         14395835 2132-03-22 20:55:00          False         False   \n",
      "2         14395835 2135-08-28 14:34:00          False         False   \n",
      "3         14395835 2135-09-08 06:31:00          False         False   \n",
      "4         14396009 2140-03-12 20:34:00          False         False   \n",
      "...            ...                 ...            ...           ...   \n",
      "214710    19999784 2123-07-17 00:00:00          False         False   \n",
      "214711    19999784 2123-08-14 08:40:00          False         False   \n",
      "214712    19999784 2123-11-12 08:49:00          False         False   \n",
      "214713    19999784 2124-03-23 00:00:00          False         False   \n",
      "214714    19999828 2149-01-08 16:44:00          False         False   \n",
      "\n",
      "        time_to_event_days  \n",
      "0              2289.216667  \n",
      "1              1614.588194  \n",
      "2               360.852778  \n",
      "3               350.188194  \n",
      "4                91.143056  \n",
      "...                    ...  \n",
      "214710          401.979861  \n",
      "214711          373.618750  \n",
      "214712          283.612500  \n",
      "214713          151.979861  \n",
      "214714           10.636111  \n",
      "\n",
      "[214715 rows x 5 columns]\n",
      "unique df1 Empty DataFrame\n",
      "Columns: [subject_id, prediction_time, boolean_value, censor_label, time_to_event_days]\n",
      "Index: []\n",
      "unique df2 Empty DataFrame\n",
      "Columns: [subject_id, prediction_time, boolean_value, features]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import pandas as pd\n",
    "import pickle\n",
    "disease_dir = Path(\"/user/zj2398/cache/motor_mimic_8k/labels\")\n",
    "feature_dir = Path(\"/user/zj2398/cache/motor_mimic_8k/embeddings\")\n",
    "model_name = \"motor\"\n",
    "\n",
    "def find_overlapping_and_unique_rows(df1, df2, columns):\n",
    "    \"\"\"\n",
    "    Find overlapping and unique rows between two DataFrames based on specified columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find overlapping rows (inner join)\n",
    "    overlapping = df1.merge(df2[columns], on=columns, how='inner')\n",
    "    \n",
    "    # Find unique rows in df1 (left only)\n",
    "    df1_with_indicator = df1.merge(df2[columns], on=columns, how='left', indicator=True)\n",
    "    unique_df1 = df1_with_indicator[df1_with_indicator['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "    \n",
    "    # Find unique rows in df2 (right only)\n",
    "    df2_with_indicator = df2.merge(df1[columns], on=columns, how='left', indicator=True)\n",
    "    unique_df2 = df2_with_indicator[df2_with_indicator['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "    \n",
    "    return overlapping, unique_df1, unique_df2\n",
    "\n",
    "disease_name = \"celiac.parquet\"\n",
    "disease_path = disease_dir /disease_name\n",
    "disease_df = pd.read_parquet(disease_path)\n",
    "print(disease_df)\n",
    "\n",
    "feature_name = \"celiac\"\n",
    "feature_path = feature_dir /feature_name/model_name/\"features_with_label\"\n",
    "train_df = pd.read_parquet(feature_path/\"train.parquet\")\n",
    "test_df = pd.read_parquet(feature_path/\"test.parquet\")\n",
    "\n",
    "# print(train_df)\n",
    "# print(test_df)\n",
    "\n",
    "df_combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# print(feature_pl.keys())\n",
    "# print(len(feature_pl[\"subject_ids\"]))\n",
    "\n",
    "# feature_df = pd.DataFrame({ \n",
    "#     \"subject_id\": feature_pl[\"subject_ids\"],\n",
    "#     \"prediction_time\":pd.to_datetime(feature_pl[\"feature_times\"]),\n",
    "#     \"features\":list(feature_pl[\"features\"])\n",
    "# })\n",
    "\n",
    "# print(f\"the length is {len(disease_df)},{len(feature_df)}\")\n",
    "\n",
    "\n",
    "overlapping,unique1,unique2 = find_overlapping_and_unique_rows(disease_df, df_combined, [\"subject_id\",\"prediction_time\",\"boolean_value\"])\n",
    "print(f\"overlapping features {overlapping}\")\n",
    "print(f\"unique df1 {unique1}\")\n",
    "print(f\"unique df2 {unique2}\")\n",
    "\n",
    "# list1= sorted(list(feature_df[\"subject_ids\"]))\n",
    "# print(len(list1),list1[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fadb0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows in disease_df: 0\n",
      "Unique (subject_id, prediction_time) in disease_df: 214715\n",
      "Total rows with duplicate keys: 310538\n",
      "Number of unique duplicate keys: 45137\n",
      "\n",
      "Complete duplicate rows (including features):\n",
      "        subject_id     prediction_time  \\\n",
      "55544     10002013 2156-11-01 14:53:00   \n",
      "474392    10002013 2156-11-01 14:53:00   \n",
      "55545     10002013 2157-10-31 12:54:00   \n",
      "105456    10002013 2157-10-31 12:54:00   \n",
      "474393    10002013 2157-10-31 12:54:00   \n",
      "...            ...                 ...   \n",
      "326562    19999784 2123-08-14 08:40:00   \n",
      "352213    19999784 2123-08-14 08:40:00   \n",
      "442096    19999784 2123-08-14 08:40:00   \n",
      "352214    19999784 2123-11-12 08:49:00   \n",
      "442097    19999784 2123-11-12 08:49:00   \n",
      "\n",
      "                                                 features  \n",
      "55544   [-1.4647691, -0.51432884, 0.01687703, 0.250716...  \n",
      "474392  [-0.15065743, -1.2854278, 0.013778948, 0.97716...  \n",
      "55545   [-0.75222325, -0.87943006, -0.1904448, 0.82533...  \n",
      "105456  [-1.4250252, -0.12956317, -0.24115029, 0.42293...  \n",
      "474393  [0.078186505, -1.4190738, 0.004506427, 0.80881...  \n",
      "...                                                   ...  \n",
      "326562  [-0.48069966, -0.6580803, 0.38521382, 1.385697...  \n",
      "352213  [-0.47582555, -0.65416634, 0.38710412, 1.38459...  \n",
      "442096  [-0.47347164, -0.6508627, 0.3850274, 1.39286, ...  \n",
      "352214  [-0.46117002, -0.6094338, 0.3686626, 1.3522006...  \n",
      "442097  [-0.46580476, -0.6050958, 0.37155005, 1.349053...  \n",
      "\n",
      "[310538 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "disease_dupes = disease_df.duplicated(subset=[\"subject_id\", \"prediction_time\"], keep=False)\n",
    "print(f\"Duplicate rows in disease_df: {disease_dupes.sum()}\")\n",
    "print(f\"Unique (subject_id, prediction_time) in disease_df: {disease_df[['subject_id', 'prediction_time']].drop_duplicates().shape[0]}\")\n",
    "\n",
    "# Check for duplicates in feature_df\n",
    "duplicate_mask = feature_df.duplicated(subset=[\"subject_id\", \"prediction_time\"], keep=False)\n",
    "duplicate_rows = feature_df[duplicate_mask].sort_values([\"subject_id\", \"prediction_time\"])\n",
    "\n",
    "print(f\"Total rows with duplicate keys: {len(duplicate_rows)}\")\n",
    "print(f\"Number of unique duplicate keys: {duplicate_rows[['subject_id', 'prediction_time']].drop_duplicates().shape[0]}\")\n",
    "print(\"\\nComplete duplicate rows (including features):\")\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0591b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original disease_df: 214715 rows -> Unique: 214715 rows\n",
      "Original feature_df: 480116 rows -> Unique: 214715 rows\n",
      "\n",
      "With unique dataframes:\n",
      "Overlapping: 214575 rows\n",
      "Unique to disease_df: 140 rows\n",
      "Unique to feature_df: 140 rows\n",
      "Total: 214715 should equal 214715\n",
      "Total: 214715 should equal 214715\n",
      "        subject_id     prediction_time  boolean_value  censor_label  \\\n",
      "2821      14534978 2123-06-07 17:28:00          False         False   \n",
      "6060      14700685 2179-08-21 19:56:00          False         False   \n",
      "7976      14787109 2120-04-23 20:12:00          False         False   \n",
      "8309      14799353 2176-01-24 21:22:00          False         False   \n",
      "9932      14873634 2161-05-20 19:29:00          False         False   \n",
      "...            ...                 ...            ...           ...   \n",
      "198008    18600217 2153-08-08 15:31:00          False         False   \n",
      "198932    18637661 2127-06-01 22:56:00          False         False   \n",
      "201319    18751419 2148-10-21 23:04:00          False         False   \n",
      "210460    19799359 2135-04-14 18:02:00          False         False   \n",
      "211062    19826668 2126-04-21 16:01:00          False         False   \n",
      "\n",
      "        time_to_event_days  \n",
      "2821            211.561736  \n",
      "6060           3657.983333  \n",
      "7976           1873.804167  \n",
      "8309           1916.464884  \n",
      "9932           1008.354861  \n",
      "...                    ...  \n",
      "198008         3054.353472  \n",
      "198932          852.493287  \n",
      "201319         2317.372222  \n",
      "210460            4.831944  \n",
      "211062          703.332639  \n",
      "\n",
      "[140 rows x 5 columns]\n",
      "        subject_id     prediction_time  \\\n",
      "94        11888614 2199-07-22 22:25:00   \n",
      "97        11888614 2199-07-28 21:30:00   \n",
      "107       11888614 2199-10-12 19:00:00   \n",
      "120       11888614 2200-04-19 21:12:00   \n",
      "401       15496609 2156-10-03 18:30:00   \n",
      "...            ...                 ...   \n",
      "203427    10804740 2185-07-07 22:39:00   \n",
      "204602    17838696 2124-06-24 13:10:00   \n",
      "204814    10145540 2168-08-28 07:00:00   \n",
      "206322    17268630 2163-01-18 22:03:00   \n",
      "207421    19826668 2126-04-21 16:00:37   \n",
      "\n",
      "                                                 features  \n",
      "94      [-0.5108506, -1.3667916, -0.094567634, 0.64306...  \n",
      "97      [-0.5083149, -1.3521042, -0.10245933, 0.682858...  \n",
      "107     [-0.74388325, -0.7441322, 0.40156528, 0.512847...  \n",
      "120     [-0.14762957, -0.97808015, 0.08983766, 1.05148...  \n",
      "401     [-1.3097787, 0.31562656, 0.052594885, 0.789037...  \n",
      "...                                                   ...  \n",
      "203427  [-1.8361899, 0.34603816, 1.5158961, 1.1655673,...  \n",
      "204602  [-1.0674281, -0.1961965, -0.12556355, 1.301224...  \n",
      "204814  [-1.0820506, -1.2210982, 0.76434124, -0.381919...  \n",
      "206322  [0.28143987, -1.3915391, 0.80653423, -0.091732...  \n",
      "207421  [-1.214027, 0.16190691, 0.26098284, 1.3245755,...  \n",
      "\n",
      "[140 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Option A: Keep first occurrence of duplicates\n",
    "disease_df_unique = disease_df.drop_duplicates(subset=[\"subject_id\", \"prediction_time\"], keep='first')\n",
    "feature_df_unique = feature_df.drop_duplicates(subset=[\"subject_id\", \"prediction_time\"], keep='first')\n",
    "\n",
    "print(f\"Original disease_df: {len(disease_df)} rows -> Unique: {len(disease_df_unique)} rows\")\n",
    "print(f\"Original feature_df: {len(feature_df)} rows -> Unique: {len(feature_df_unique)} rows\")\n",
    "\n",
    "# Option B: Keep last occurrence of duplicates\n",
    "# disease_df_unique = disease_df.drop_duplicates(subset=[\"subject_id\", \"prediction_time\"], keep='last')\n",
    "# feature_df_unique = feature_df.drop_duplicates(subset=[\"subject_id\", \"prediction_time\"], keep='last')\n",
    "\n",
    "# Option C: Remove ALL duplicates (keep none of them)\n",
    "# disease_df_unique = disease_df[~disease_df.duplicated(subset=[\"subject_id\", \"prediction_time\"], keep=False)]\n",
    "# feature_df_unique = feature_df[~feature_df.duplicated(subset=[\"subject_id\", \"prediction_time\"], keep=False)]\n",
    "\n",
    "# Now run your overlap analysis with unique dataframes\n",
    "overlapping, unique1, unique2 = find_overlapping_and_unique_rows(\n",
    "    disease_df_unique, \n",
    "    feature_df_unique, \n",
    "    [\"subject_id\", \"prediction_time\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nWith unique dataframes:\")\n",
    "print(f\"Overlapping: {len(overlapping)} rows\")\n",
    "print(f\"Unique to disease_df: {len(unique1)} rows\")\n",
    "print(f\"Unique to feature_df: {len(unique2)} rows\")\n",
    "print(f\"Total: {len(overlapping) + len(unique1)} should equal {len(disease_df_unique)}\")\n",
    "print(f\"Total: {len(overlapping) + len(unique2)} should equal {len(feature_df_unique)}\")\n",
    "print(unique1)\n",
    "print(unique2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2ab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/zj2398/cache/motor_mimic_8k/output/features/in_hospital_mortality_motor.pkl\n",
      "476019\n",
      "476019\n",
      "/user/zj2398/cache/motor_mimic_8k/output/features/celiac_motor.pkl\n",
      "480116\n",
      "480116\n",
      "/user/zj2398/cache/motor_mimic_8k/output/features/long_los_motor.pkl\n",
      "476006\n",
      "476006\n",
      "/user/zj2398/cache/motor_mimic_8k/output/features/readmission_motor.pkl\n",
      "703273\n",
      "703273\n",
      "/user/zj2398/cache/motor_mimic_8k/output/features/stroke_motor.pkl\n",
      "1437924\n",
      "1437924\n",
      "/user/zj2398/cache/motor_mimic_8k/output/features/masld_motor.pkl\n",
      "405057\n",
      "405057\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import os \n",
    "phenotype_path = \"/user/zj2398/cache/mtpp_8k/mtpp_mean_all/features\"\n",
    "exclude_ids = [11704827,14129581,17454346,19824820]\n",
    "# phenotype_path = \"/user/zj2398/cache/mimic/mimic-3.1-meds/phenotype_task/\"\n",
    "all_files = glob.glob(os.path.join(phenotype_path, '**', '*.pkl'), recursive=True)\n",
    "for file_path in all_files:\n",
    "    print(file_path)\n",
    "    with open(file_path,'rb') as file:\n",
    "        disease_df = pickle.load(file)\n",
    "        # print(disease_df.keys())\n",
    "        print(len(disease_df['subject_ids']))\n",
    "        mask = ~np.isin(disease_df['subject_ids'], exclude_ids)\n",
    "        new_dict = {\n",
    "            'subject_ids': disease_df['subject_ids'][mask],\n",
    "            'feature_times': disease_df['feature_times'][mask], \n",
    "            'features': disease_df['features'][mask]\n",
    "        }\n",
    "        print(len(disease_df['subject_ids'][mask]))\n",
    "        # with open(file_path, 'wb') as f:\n",
    "        #     pickle.dump(new_dict, f)\n",
    "        \n",
    "    # disease_df = disease_df.rename(columns={\"binary_label\":\"boolean_value\"})\n",
    "    # disease_df.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e25a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        subject_id     prediction_time  boolean_value  \\\n",
      "0         12754657 2142-03-24 19:57:00          False   \n",
      "1         10009326 2116-03-31 00:31:00          False   \n",
      "2         10722703 2132-06-11 05:07:00          False   \n",
      "3         17578973 2168-12-14 03:44:00          False   \n",
      "4         13080738 2133-07-31 20:17:00          False   \n",
      "...            ...                 ...            ...   \n",
      "281658    14244279 2146-02-02 20:41:00          False   \n",
      "281659    19168752 2189-11-26 16:15:00          False   \n",
      "281660    14685940 2127-07-31 14:46:00          False   \n",
      "281661    15229574 2125-03-10 23:37:00          False   \n",
      "281662    14313330 2147-05-04 21:20:00          False   \n",
      "\n",
      "                                                 features  time_to_event_days  \n",
      "0       [-1.0309983, 1.1677849, -0.30635804, 0.0384228...          251.168750  \n",
      "1       [1.1620811, 1.5489517, -2.0962741, 1.7200605, ...            9.978472  \n",
      "2       [0.3266717, 1.5968416, -0.8996342, 0.48808944,...         1378.402685  \n",
      "3       [-0.6976077, 1.2039477, -1.0641123, 1.9904385,...          316.431250  \n",
      "4       [-1.1109985, -0.6169655, -0.2853634, -0.853487...          536.154861  \n",
      "...                                                   ...                 ...  \n",
      "281658  [1.1619347, 1.252091, -1.1136101, 1.3099998, 1...         1832.638194  \n",
      "281659  [0.2114619, 1.06434, -2.4741518, 2.0127885, 1....          690.905382  \n",
      "281660  [-0.9250524, -2.2397268, 0.9461936, 0.47907868...         1040.718056  \n",
      "281661  [0.9123289, 2.2914681, -1.183999, -0.81985295,...         2033.015972  \n",
      "281662  [0.46674135, 0.97828007, -0.47262886, -0.42950...          816.725694  \n",
      "\n",
      "[281663 rows x 5 columns]\n",
      "       subject_id     prediction_time  boolean_value  \\\n",
      "0        10002221 2203-12-30 12:21:00          False   \n",
      "1        10002221 2204-06-27 16:57:00          False   \n",
      "2        10002221 2204-07-03 11:45:00          False   \n",
      "3        10002221 2204-07-03 22:34:00          False   \n",
      "4        10002221 2204-07-16 11:39:00          False   \n",
      "...           ...                 ...            ...   \n",
      "32531    19998330 2178-11-18 23:33:00          False   \n",
      "32532    19998330 2178-11-27 19:21:00          False   \n",
      "32533    19998330 2178-11-27 21:51:00          False   \n",
      "32534    19998330 2178-11-27 22:53:00          False   \n",
      "32535    19998330 2178-11-29 21:51:19          False   \n",
      "\n",
      "                                                features  time_to_event_days  \n",
      "0      [-0.068334974, 1.4898237, 0.29719907, 0.694096...         1082.997917  \n",
      "1      [-1.1571676, 1.0271944, -0.4795328, 1.7742524,...          902.806250  \n",
      "2      [-0.4494862, 0.92185396, -0.092470124, 1.50421...          897.022917  \n",
      "3      [-0.5977858, 0.6819075, -0.63687754, 1.9164785...          896.572222  \n",
      "4      [-0.57893336, 0.88729376, -0.23222159, 1.64921...          884.027083  \n",
      "...                                                  ...                 ...  \n",
      "32531  [-0.03145494, 0.07929867, 1.209682, 0.10075643...           20.018750  \n",
      "32532  [0.030787136, -0.61821353, 1.536746, 0.8457455...           11.193750  \n",
      "32533  [0.33604044, -0.5640281, 1.1253675, 0.967621, ...           11.089583  \n",
      "32534  [0.025627302, -0.9713896, 1.3276851, 0.7812983...           11.046528  \n",
      "32535  [-0.22304545, -0.47755194, 1.5952246, 0.399637...            9.089363  \n",
      "\n",
      "[32536 rows x 5 columns]\n",
      "        subject_id     prediction_time  boolean_value  \\\n",
      "0         14856789 2159-07-01 01:39:00          False   \n",
      "1         17714334 2154-05-11 16:30:00          False   \n",
      "2         18828728 2156-06-07 09:19:00          False   \n",
      "3         18613771 2135-06-16 07:15:00          False   \n",
      "4         11719852 2125-10-24 15:09:00          False   \n",
      "...            ...                 ...            ...   \n",
      "187516    16383947 2164-06-30 01:47:00          False   \n",
      "187517    15514336 2153-11-09 00:46:00          False   \n",
      "187518    17002650 2154-06-10 21:50:00          False   \n",
      "187519    17821868 2180-12-09 17:33:00          False   \n",
      "187520    16486571 2201-08-16 02:57:00          False   \n",
      "\n",
      "                                                 features  time_to_event_days  \n",
      "0       [-0.81256163, 0.09338329, -0.77497506, 1.73485...           60.931250  \n",
      "1       [-1.6759951, 1.7864869, 0.11399857, 1.0522957,...          436.744444  \n",
      "2       [-0.77201957, 1.2058021, -0.44299135, 0.636509...         3254.106250  \n",
      "3       [-1.3570073, 1.5016088, -0.023109365, 0.005771...         4169.697917  \n",
      "4       [0.060719736, -0.538253, -0.5489207, 1.6244895...           43.368750  \n",
      "...                                                   ...                 ...  \n",
      "187516  [0.48297068, 0.5652487, 0.8313463, -0.5837561,...         2755.395139  \n",
      "187517  [-0.5997072, 1.0708477, 1.1334537, 0.09449627,...         3236.968056  \n",
      "187518  [-0.5358748, -1.2601724, -1.0187278, 1.3216285...            2.881944  \n",
      "187519  [-1.3388371, 1.0393251, -0.12223328, 0.9877525...          778.035417  \n",
      "187520  [-0.8250077, 2.2638512, 0.32372096, -0.2750430...         3517.373611  \n",
      "\n",
      "[187521 rows x 5 columns]\n",
      "       subject_id     prediction_time  boolean_value  \\\n",
      "0        10002221 2204-06-27 16:57:00           True   \n",
      "1        10003637 2145-11-23 07:15:00          False   \n",
      "2        10003637 2146-01-22 23:08:00          False   \n",
      "3        10003637 2146-02-18 16:49:00          False   \n",
      "4        10003637 2149-05-13 15:41:00          False   \n",
      "...           ...                 ...            ...   \n",
      "21632    19998330 2178-09-20 20:20:00          False   \n",
      "21633    19998330 2178-10-01 07:28:00          False   \n",
      "21634    19998330 2178-10-10 15:23:00          False   \n",
      "21635    19998330 2178-10-21 15:14:00          False   \n",
      "21636    19998330 2178-11-27 21:51:00          False   \n",
      "\n",
      "                                                features  time_to_event_days  \n",
      "0      [-1.1571676, 1.0271944, -0.4795328, 1.7742524,...            2.830556  \n",
      "1      [0.8629135, 1.9000895, -0.7785396, 0.42618233,...         1641.656250  \n",
      "2      [0.98470867, 1.6819824, -1.3617259, 1.3521317,...         1580.994444  \n",
      "3      [1.612437, 1.649735, -1.422951, 1.4336216, 1.3...         1554.257639  \n",
      "4      [-0.4142071, 0.21552734, -0.7119818, 0.7775594...          374.304861  \n",
      "...                                                  ...                 ...  \n",
      "21632  [0.15210643, -0.77253664, 0.52066, 2.1636205, ...           79.152778  \n",
      "21633  [0.23874414, -1.1151481, 1.070131, 1.9494451, ...           68.688889  \n",
      "21634  [0.10699423, -0.951308, 0.86325145, 1.6662245,...           59.359028  \n",
      "21635  [0.06788901, -0.95876634, 1.4919431, 0.6424573...           48.365278  \n",
      "21636  [0.33604044, -0.5640281, 1.1253675, 0.967621, ...           11.089583  \n",
      "\n",
      "[21637 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# fetch tte labels from phenotype task to processed task\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import femr\n",
    "import femr.featurizers\n",
    "import femr.splits\n",
    "\n",
    "diseases = [\"stroke\",\"masld\"]\n",
    "# phenotype_dir = Path(\"/data/processed_datasets/processed_datasets/mimic/phenotype_task\")\n",
    "# file1 = pd.read_parquet('/data/processed_datasets/processed_datasets/mimic/phenotype_task/celiac/celiac_cohort.parquet')\n",
    "phenotype_dir = Path(\"/user/zj2398/cache/mimic/mimic-3.1-meds/phenotype_task\")\n",
    "embedding_dir = Path(\"/user/zj2398/cache/motor_mimic_8k/results\")\n",
    "for disease in diseases:\n",
    "    df1 = pd.read_parquet(phenotype_dir / disease / f\"{disease}_cohort.parquet\")\n",
    "    # print(cohort_label.head())\n",
    "\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        split_path = embedding_dir / disease/ f\"motor/features_with_label/{split}.parquet\"\n",
    "        with open(split_path, 'rb') as f:\n",
    "            df2 = pd.read_parquet(f)\n",
    "        print(df2)\n",
    "\n",
    "\n",
    "        # --- 0) (Optional) make copies so originals stay intact\n",
    "        df1_ = df1.copy()\n",
    "        df2_ = df2.copy()\n",
    "\n",
    "        # --- 1) Ensure dtypes match exactly\n",
    "        # prediction_time to datetime (naive)\n",
    "        df1_[\"prediction_time\"] = pd.to_datetime(df1_[\"prediction_time\"], errors=\"raise\", utc=False)\n",
    "        df2_[\"prediction_time\"] = pd.to_datetime(df2_[\"prediction_time\"], errors=\"raise\", utc=False)\n",
    "\n",
    "        # boolean_value to proper bool\n",
    "        # (handles cases where it's already bool or string \"True\"/\"False\")\n",
    "        df1_[\"boolean_value\"] = df1_[\"boolean_value\"].astype(str).str.lower().map({\"true\": True, \"false\": False})\n",
    "        df2_[\"boolean_value\"] = df2_[\"boolean_value\"].astype(str).str.lower().map({\"true\": True, \"false\": False})\n",
    "\n",
    "        # subject_id to consistent integer (or string if you prefer)\n",
    "        df1_[\"subject_id\"] = pd.to_numeric(df1_[\"subject_id\"], downcast=\"integer\")\n",
    "        df2_[\"subject_id\"] = pd.to_numeric(df2_[\"subject_id\"], downcast=\"integer\")\n",
    "\n",
    "        # --- 2) Check that the join key is unique on the right side (df1)\n",
    "        key_cols = [\"subject_id\", \"prediction_time\", \"boolean_value\"]\n",
    "        dupes = df1_.duplicated(subset=key_cols, keep=False)\n",
    "        if dupes.any():\n",
    "            # Show a few offending keys to help diagnose\n",
    "            raise ValueError(\n",
    "                \"df1 has duplicate rows for the join key; expected 1 row per key.\\n\"\n",
    "                f\"Examples:\\n{df1_.loc[dupes, key_cols].head(10)}\"\n",
    "            )\n",
    "\n",
    "        # --- 3) Merge (df2 left-join to df1), enforcing m:1 cardinality\n",
    "        merged = df2_.merge(\n",
    "            df1_[key_cols + [\"time_to_event_days\"]],\n",
    "            on=key_cols,\n",
    "            how=\"left\",\n",
    "            validate=\"m:1\"\n",
    "        )\n",
    "\n",
    "        # --- 4) Verify every df2 row matched something in df1\n",
    "        num_unmatched = merged[\"time_to_event_days\"].isna().sum()\n",
    "        if num_unmatched:\n",
    "            # Show a few missing keys to debug\n",
    "            missing_keys = merged.loc[merged[\"time_to_event_days\"].isna(), key_cols].drop_duplicates().head(10)\n",
    "            raise ValueError(\n",
    "                f\"{num_unmatched} row(s) in df2 did not find a match in df1. \"\n",
    "                f\"Examples of missing keys:\\n{missing_keys}\"\n",
    "            )\n",
    "\n",
    "        # --- 5) If you only want df2 + the added label:\n",
    "        df2_with_label = merged  # same columns as df2, plus time_to_event_days\n",
    "        # print(df2_with_label)\n",
    "        df2_with_label.to_parquet(split_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch tte labels from phenotype task to processed task\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import femr\n",
    "import femr.featurizers\n",
    "import femr.splits\n",
    "\n",
    "diseases = [\"stroke\"]\n",
    "# phenotype_dir = Path(\"/data/processed_datasets/processed_datasets/mimic/phenotype_task\")\n",
    "# file1 = pd.read_parquet('/data/processed_datasets/processed_datasets/mimic/phenotype_task/celiac/celiac_cohort.parquet')\n",
    "phenotype_dir = Path(\"/user/zj2398/cache/mimic/mimic-3.1-meds/phenotype_task\")\n",
    "embedding_dir = Path(\"/user/zj2398/cache/motor_mimic_8k/output/features\")\n",
    "for disease in diseases:\n",
    "    df1 = pd.read_parquet(phenotype_dir / disease / f\"{disease}_cohort.parquet\")\n",
    "    # print(cohort_label.head())\n",
    "\n",
    "    embedding_path = embedding_dir / disease\n",
    "    with open(split_path, 'rb') as f:\n",
    "        df2 = pd.read_parquet(f)\n",
    "    print(df2)\n",
    "\n",
    "\n",
    "    # --- 0) (Optional) make copies so originals stay intact\n",
    "    df1_ = df1.copy()\n",
    "    df2_ = df2.copy()\n",
    "\n",
    "    # --- 1) Ensure dtypes match exactly\n",
    "    # prediction_time to datetime (naive)\n",
    "    df1_[\"prediction_time\"] = pd.to_datetime(df1_[\"prediction_time\"], errors=\"raise\", utc=False)\n",
    "    df2_[\"prediction_time\"] = pd.to_datetime(df2_[\"prediction_time\"], errors=\"raise\", utc=False)\n",
    "\n",
    "    # boolean_value to proper bool\n",
    "    # (handles cases where it's already bool or string \"True\"/\"False\")\n",
    "    df1_[\"boolean_value\"] = df1_[\"boolean_value\"].astype(str).str.lower().map({\"true\": True, \"false\": False})\n",
    "    df2_[\"boolean_value\"] = df2_[\"boolean_value\"].astype(str).str.lower().map({\"true\": True, \"false\": False})\n",
    "\n",
    "    # subject_id to consistent integer (or string if you prefer)\n",
    "    df1_[\"subject_id\"] = pd.to_numeric(df1_[\"subject_id\"], downcast=\"integer\")\n",
    "    df2_[\"subject_id\"] = pd.to_numeric(df2_[\"subject_id\"], downcast=\"integer\")\n",
    "\n",
    "    # --- 2) Check that the join key is unique on the right side (df1)\n",
    "    key_cols = [\"subject_id\", \"prediction_time\", \"boolean_value\"]\n",
    "    dupes = df1_.duplicated(subset=key_cols, keep=False)\n",
    "    if dupes.any():\n",
    "        # Show a few offending keys to help diagnose\n",
    "        raise ValueError(\n",
    "            \"df1 has duplicate rows for the join key; expected 1 row per key.\\n\"\n",
    "            f\"Examples:\\n{df1_.loc[dupes, key_cols].head(10)}\"\n",
    "        )\n",
    "\n",
    "    # --- 3) Merge (df2 left-join to df1), enforcing m:1 cardinality\n",
    "    merged = df2_.merge(\n",
    "        df1_[key_cols + [\"time_to_event_days\"]],\n",
    "        on=key_cols,\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\"\n",
    "    )\n",
    "\n",
    "    # --- 4) Verify every df2 row matched something in df1\n",
    "    num_unmatched = merged[\"time_to_event_days\"].isna().sum()\n",
    "    if num_unmatched:\n",
    "        # Show a few missing keys to debug\n",
    "        missing_keys = merged.loc[merged[\"time_to_event_days\"].isna(), key_cols].drop_duplicates().head(10)\n",
    "        raise ValueError(\n",
    "            f\"{num_unmatched} row(s) in df2 did not find a match in df1. \"\n",
    "            f\"Examples of missing keys:\\n{missing_keys}\"\n",
    "        )\n",
    "\n",
    "    # --- 5) If you only want df2 + the added label:\n",
    "    df2_with_label = merged  # same columns as df2, plus time_to_event_days\n",
    "    # print(df2_with_label)\n",
    "    df2_with_label.to_parquet(split_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_dupes = disease_df.duplicated(subset=[\"subject_id\", \"prediction_time\"], keep=False)\n",
    "print(f\"Duplicate rows in disease_df: {disease_dupes.sum()}\")\n",
    "print(f\"Unique (subject_id, prediction_time) in disease_df: {disease_df[['subject_id', 'prediction_time']].drop_duplicates().shape[0]}\")\n",
    "\n",
    "# Check for duplicates in feature_df\n",
    "duplicate_mask = feature_df.duplicated(subset=[\"subject_id\", \"prediction_time\"], keep=False)\n",
    "duplicate_rows = feature_df[duplicate_mask].sort_values([\"subject_id\", \"prediction_time\"])\n",
    "\n",
    "print(f\"Total rows with duplicate keys: {len(duplicate_rows)}\")\n",
    "print(f\"Number of unique duplicate keys: {duplicate_rows[['subject_id', 'prediction_time']].drop_duplicates().shape[0]}\")\n",
    "print(\"\\nComplete duplicate rows (including features):\")\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509da4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
