{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe7a3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422757\n",
      "422757\n",
      "1156492\n",
      "1156492\n",
      "3231218\n",
      "3231218\n",
      "3129231\n",
      "3129231\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "disease_dir = Path(\"/shared/share_mala/zj2398/mimic/regression_labels_nona/\")\n",
    "for parquet_file in disease_dir.glob(\"*.parquet\"):\n",
    "    disease_file = pd.read_parquet(parquet_file)\n",
    "    # disease_file = disease_file.dropna(subset=[\"numeric_value\"])\n",
    "    # disease_file.to_parquet(parquet_file)\n",
    "    print(len(disease_file))\n",
    "    # # print(disease_file)\n",
    "    mask_nan = disease_file[\"numeric_value\"].notna()\n",
    "    print(sum(mask_nan))\n",
    "    # print(disease_file.iloc[511934][\"numeric_value\"].dtype())\n",
    "    # print(disease_file[\"numeric_value\"] != float('nan'))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e195ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pao2_regression.parquet\n",
      "parquet_file 422757\n",
      "        subject_id     prediction_time  target_value   unit\n",
      "0         10000935 2187-10-22 15:42:00          86.0  mm Hg\n",
      "2         10001884 2130-10-10 09:32:00          73.0  mm Hg\n",
      "21        10002013 2160-05-18 09:23:00         441.0  mm Hg\n",
      "34        10002155 2129-08-04 12:27:00          89.0  mm Hg\n",
      "45        10002428 2156-04-19 19:43:00          95.0  mm Hg\n",
      "...            ...                 ...           ...    ...\n",
      "511913    19999287 2197-08-06 18:51:00         217.0  mm Hg\n",
      "511917    19999442 2148-11-19 17:15:00         131.0  mm Hg\n",
      "511921    19999625 2139-10-10 17:31:00          71.0  mm Hg\n",
      "511922    19999828 2149-01-08 10:06:00          77.0  mm Hg\n",
      "511926    19999840 2164-09-12 09:48:00         212.0  mm Hg\n",
      "\n",
      "[27446 rows x 4 columns]\n",
      "Kept 27,446 of 422,715 rows within 1 year of first event.\n",
      "bilirubin_regression.parquet\n",
      "parquet_file 1156492\n",
      "         subject_id     prediction_time  target_value   unit\n",
      "1          10000032 2180-05-06 23:16:00           1.6  mg/dL\n",
      "16         10000084 2160-12-28 05:02:00           0.6  mg/dL\n",
      "18         10000117 2176-02-08 22:43:00           1.1  mg/dL\n",
      "25         10000285 2161-11-08 17:44:00           0.7  mg/dL\n",
      "27         10000560 2189-06-26 14:30:00           0.3  mg/dL\n",
      "...             ...                 ...           ...    ...\n",
      "1174777    19999625 2138-10-06 15:21:00           0.7  mg/dL\n",
      "1174779    19999636 2187-09-15 19:28:00           0.5  mg/dL\n",
      "1174784    19999784 2119-07-31 09:56:00           0.3  mg/dL\n",
      "1174891    19999828 2149-01-08 10:47:00           0.4  mg/dL\n",
      "1174896    19999840 2164-09-12 09:10:00           0.8  mg/dL\n",
      "\n",
      "[95607 rows x 4 columns]\n",
      "Kept 95,607 of 1,156,381 rows within 1 year of first event.\n",
      "creatinine_regression.parquet\n",
      "parquet_file 3231218\n",
      "         subject_id     prediction_time  target_value   unit\n",
      "1          10000032 2180-05-06 23:16:00           0.3  mg/dL\n",
      "24         10000084 2160-12-27 20:13:00           0.8  mg/dL\n",
      "27         10000117 2176-02-08 22:43:00           0.9  mg/dL\n",
      "45         10000285 2161-11-08 16:52:00           0.8  mg/dL\n",
      "48         10000560 2189-06-26 14:30:00           0.6  mg/dL\n",
      "...             ...                 ...           ...    ...\n",
      "3282060    19999659 2182-01-07 23:14:00           0.6  mg/dL\n",
      "3282074    19999784 2119-07-24 23:41:00           0.9  mg/dL\n",
      "3282236    19999828 2149-01-08 10:47:00           0.8  mg/dL\n",
      "3282258    19999840 2164-09-11 08:21:00           0.6  mg/dL\n",
      "3282277    19999987 2146-02-07 16:26:00           1.1  mg/dL\n",
      "\n",
      "[141214 rows x 4 columns]\n",
      "Kept 141,214 of 3,230,981 rows within 1 year of first event.\n",
      "platelets_regression.parquet\n",
      "parquet_file 3129231\n",
      "         subject_id     prediction_time  target_value  unit\n",
      "1          10000032 2180-05-06 22:42:00          71.0  K/uL\n",
      "19         10000084 2160-12-27 19:43:00         257.0  K/uL\n",
      "22         10000117 2176-02-08 22:24:00         307.0  K/uL\n",
      "45         10000285 2161-11-08 16:17:00         155.0  K/uL\n",
      "48         10000560 2189-06-26 13:44:00         222.0  K/uL\n",
      "...             ...                 ...           ...   ...\n",
      "3216446    19999636 2187-09-15 18:46:00         295.0  K/uL\n",
      "3216448    19999659 2182-01-07 23:08:00         132.0  K/uL\n",
      "3216463    19999784 2119-07-24 22:18:00         268.0  K/uL\n",
      "3216624    19999828 2149-01-08 10:14:00         548.0  K/uL\n",
      "3216640    19999840 2164-09-11 08:00:00         272.0  K/uL\n",
      "\n",
      "[144128 rows x 4 columns]\n",
      "Kept 144,128 of 3,128,970 rows within 1 year of first event.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import meds_reader\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "disease_dir = Path(\"/shared/share_mala/zj2398/mimic/regression_labels_nona/\")\n",
    "task=\"bilirubin\"\n",
    "meds_path = \"/user/zj2398/cache/mimic/meds_v0.6_reader\"\n",
    "\n",
    "disease_path = disease_dir/f\"{task}_regression.parquet\"\n",
    "# df = pd.read_parquet(disease_path)\n",
    "\n",
    "database = meds_reader.SubjectDatabase(meds_path)\n",
    "# print(database[10000032])\n",
    "# print(disease_file)\n",
    "# Ensure types\n",
    "for parquet_file in disease_dir.glob(\"*.parquet\"):\n",
    "    print(parquet_file.name)\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    print(f\"parquet_file {len(df)}\")\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(int)\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "\n",
    "    # 2) Build unique patient list from the dataframe (so we only fetch what we need)\n",
    "    all_pids = df[\"patient_id\"].unique().tolist()\n",
    "\n",
    "    # --- Worker function: open SubjectDatabase inside the process, compute first non-None event time per patient ---\n",
    "    def first_event_times_for_chunk(pids_chunk, meds_path_str):\n",
    "        from datetime import datetime\n",
    "        import meds_reader as _mr\n",
    "\n",
    "        db = _mr.SubjectDatabase(meds_path_str)\n",
    "\n",
    "        out = {}\n",
    "        for pid in pids_chunk:\n",
    "            try:\n",
    "                subj = db[pid]\n",
    "            except KeyError:\n",
    "                # no subject -> skip\n",
    "                continue\n",
    "            first_t = None\n",
    "            for ev in subj.events:\n",
    "                # ev.time can be None; keep the earliest non-None\n",
    "                if ev.time is not None and ev.code != \"MEDS_BIRTH\":\n",
    "                    first_t = ev.time\n",
    "                    break\n",
    "                    # if (first_t is None) or (ev.time < first_t):\n",
    "                    #     first_t = ev.time\n",
    "            if first_t is not None:\n",
    "                out[pid] = pd.Timestamp(first_t)  # normalize to pandas Timestamp\n",
    "        return out\n",
    "\n",
    "    # 3) Parallelize over patient chunks (each worker builds its own SubjectDatabase handle)\n",
    "    def chunked(iterable, size):\n",
    "        for i in range(0, len(iterable), size):\n",
    "            yield iterable[i:i+size]\n",
    "\n",
    "    pid_chunks = list(chunked(all_pids, 1000))\n",
    "    pid_to_first = {}\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=128) as ex:\n",
    "        futs = [ex.submit(first_event_times_for_chunk, ch, str(meds_path)) for ch in pid_chunks]\n",
    "        for fut in as_completed(futs):\n",
    "            pid_to_first.update(fut.result())\n",
    "\n",
    "    # 4) Map first event times back to the dataframe\n",
    "    df[\"first_event_time\"] = df[\"patient_id\"].map(pid_to_first)\n",
    "\n",
    "    # Drop rows for patients with no valid first event time\n",
    "    df = df.dropna(subset=[\"first_event_time\"])\n",
    "\n",
    "    # 5) Keep only rows within the first year after the first event (and not earlier)\n",
    "    one_month = pd.Timedelta(days=30)\n",
    "    mask = df[\"time\"] >= (df[\"first_event_time\"]+  one_month)\n",
    "    # print(df)\n",
    "    filtered = df.loc[mask, [\"patient_id\", \"time\", \"numeric_value\", \"unit\"]]  # keep any columns you want\n",
    "    filtered = filtered.sort_values([\"patient_id\", \"time\"]).drop_duplicates(\"patient_id\", keep=\"first\")\n",
    "    filtered = filtered.rename(columns={\n",
    "    \"patient_id\": \"subject_id\",\n",
    "    \"time\": \"prediction_time\",\n",
    "    \"numeric_value\":\"target_value\"\n",
    "    })\n",
    "    print(filtered)\n",
    "    # (Optional) Save\n",
    "    # filtered.to_parquet(\"/path/to/filtered.parquet\", index=False)\n",
    "    store_path = \"/shared/share_mala/zj2398/mimic/regression_labels_1_month/\"+ parquet_file.name\n",
    "    filtered.to_parquet(store_path)\n",
    "\n",
    "    print(f\"Kept {len(filtered):,} of {len(df):,} rows within 1 year of first event.\")\n",
    "\n",
    "# print(disease_file.iloc[511934][\"numeric_value\"].dtype())\n",
    "# print(disease_file[\"numeric_value\"] != float('nan'))\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50cf5e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pao2_regression.parquet\n",
      "parquet_file 422757\n",
      "Kept 46,953 of 422,715 rows within 1 year of first event.\n",
      "bilirubin_regression.parquet\n",
      "parquet_file 1156492\n",
      "Kept 145,236 of 1,156,381 rows within 1 year of first event.\n",
      "creatinine_regression.parquet\n",
      "parquet_file 3231218\n",
      "Kept 239,518 of 3,230,981 rows within 1 year of first event.\n",
      "platelets_regression.parquet\n",
      "parquet_file 3129231\n",
      "Kept 242,041 of 3,128,970 rows within 1 year of first event.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import meds_reader\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "disease_dir = Path(\"/shared/share_mala/zj2398/mimic/regression_labels_nona/\")\n",
    "task=\"bilirubin\"\n",
    "meds_path = \"/user/zj2398/cache/mimic/meds_v0.6_reader\"\n",
    "\n",
    "disease_path = disease_dir/f\"{task}_regression.parquet\"\n",
    "# df = pd.read_parquet(disease_path)\n",
    "\n",
    "database = meds_reader.SubjectDatabase(meds_path)\n",
    "# print(database[10000032])\n",
    "# print(disease_file)\n",
    "# Ensure types\n",
    "for parquet_file in disease_dir.glob(\"*.parquet\"):\n",
    "    print(parquet_file.name)\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    print(f\"parquet_file {len(df)}\")\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(int)\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "\n",
    "    # 2) Build unique patient list from the dataframe (so we only fetch what we need)\n",
    "    all_pids = df[\"patient_id\"].unique().tolist()\n",
    "\n",
    "    # --- Worker function: open SubjectDatabase inside the process, compute first non-None event time per patient ---\n",
    "    def first_event_times_for_chunk(pids_chunk, meds_path_str):\n",
    "        from datetime import datetime\n",
    "        import meds_reader as _mr\n",
    "\n",
    "        db = _mr.SubjectDatabase(meds_path_str)\n",
    "\n",
    "        out = {}\n",
    "        for pid in pids_chunk:\n",
    "            try:\n",
    "                subj = db[pid]\n",
    "            except KeyError:\n",
    "                # no subject -> skip\n",
    "                continue\n",
    "            first_t = None\n",
    "            for ev in subj.events:\n",
    "                # ev.time can be None; keep the earliest non-None\n",
    "                if ev.time is not None and ev.code != \"MEDS_BIRTH\":\n",
    "                    first_t = ev.time\n",
    "                    break\n",
    "                    # if (first_t is None) or (ev.time < first_t):\n",
    "                    #     first_t = ev.time\n",
    "            if first_t is not None:\n",
    "                out[pid] = pd.Timestamp(first_t)  # normalize to pandas Timestamp\n",
    "        return out\n",
    "\n",
    "    # 3) Parallelize over patient chunks (each worker builds its own SubjectDatabase handle)\n",
    "    def chunked(iterable, size):\n",
    "        for i in range(0, len(iterable), size):\n",
    "            yield iterable[i:i+size]\n",
    "\n",
    "    pid_chunks = list(chunked(all_pids, 1000))\n",
    "    pid_to_first = {}\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=128) as ex:\n",
    "        futs = [ex.submit(first_event_times_for_chunk, ch, str(meds_path)) for ch in pid_chunks]\n",
    "        for fut in as_completed(futs):\n",
    "            pid_to_first.update(fut.result())\n",
    "\n",
    "    # 4) Map first event times back to the dataframe\n",
    "    df[\"first_event_time\"] = df[\"patient_id\"].map(pid_to_first)\n",
    "\n",
    "    # Drop rows for patients with no valid first event time\n",
    "    df = df.dropna(subset=[\"first_event_time\"])\n",
    "\n",
    "    # 5) Keep only rows within the first year after the first event (and not earlier)\n",
    "    one_year = pd.Timedelta(days=0)\n",
    "    mask = df[\"time\"] >= (df[\"first_event_time\"]+ one_year)\n",
    "    filtered = df.loc[mask, [\"patient_id\", \"time\", \"code\", \"unit\"]]  # keep any columns you want\n",
    "    filtered = filtered.sort_values([\"patient_id\", \"time\"]).drop_duplicates(\"patient_id\", keep=\"first\")\n",
    "\n",
    "    # (Optional) Save\n",
    "    # filtered.to_parquet(\"/path/to/filtered.parquet\", index=False)\n",
    "    # store_path = \"/shared/share_mala/zj2398/mimic/regression_labels_2_years/\"+ parquet_file.name\n",
    "    # filtered.to_parquet(store_path)\n",
    "\n",
    "    print(f\"Kept {len(filtered):,} of {len(df):,} rows within 1 year of first event.\")\n",
    "\n",
    "# print(disease_file.iloc[511934][\"numeric_value\"].dtype())\n",
    "# print(disease_file[\"numeric_value\"] != float('nan'))\n",
    "# break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
