{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a154d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_hospital_mortality\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "def read_recursive_parquet(root_dir):\n",
    "    all_files = glob.glob(os.path.join(root_dir, '**', '*.parquet'), recursive=True)\n",
    "    print(all_files)\n",
    "    df = pd.concat((pd.read_parquet(f) for f in all_files), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "cohort_dir = \"/data/processed_datasets/processed_datasets/mimic/mimic_3.1/patient_outcome_tasks/in_hospital_mortality\"\n",
    "# cohort = read_recursive_parquet(cohort_dir)\n",
    "# print(cohort.prediction_time)\n",
    "label_name = os.path.basename(os.path.normpath(cohort_dir))\n",
    "print(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e2ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# phenotype_path = \"/data/processed_datasets/processed_datasets/mimic/phenotype_task\"\n",
    "phenotype_path = \"/user/zj2398/cache/mimic/mimic-3.1-meds/phenotype_task/\"\n",
    "all_files = glob.glob(os.path.join(phenotype_path, '**', '*.parquet'), recursive=True)\n",
    "for file_path in all_files:\n",
    "    disease_df = pd.read_parquet(file_path)\n",
    "    disease_df = disease_df.rename(columns={\"binary_label\":\"boolean_value\"})\n",
    "    disease_df.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e25a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        subject_id     prediction_time  boolean_value  \\\n",
      "0         12754657 2142-03-24 19:57:00          False   \n",
      "1         10009326 2116-03-31 00:31:00          False   \n",
      "2         10722703 2132-06-11 05:07:00          False   \n",
      "3         17578973 2168-12-14 03:44:00          False   \n",
      "4         13080738 2133-07-31 20:17:00          False   \n",
      "...            ...                 ...            ...   \n",
      "281658    14244279 2146-02-02 20:41:00          False   \n",
      "281659    19168752 2189-11-26 16:15:00          False   \n",
      "281660    14685940 2127-07-31 14:46:00          False   \n",
      "281661    15229574 2125-03-10 23:37:00          False   \n",
      "281662    14313330 2147-05-04 21:20:00          False   \n",
      "\n",
      "                                                 features  time_to_event_days  \n",
      "0       [-1.0309983, 1.1677849, -0.30635804, 0.0384228...          251.168750  \n",
      "1       [1.1620811, 1.5489517, -2.0962741, 1.7200605, ...            9.978472  \n",
      "2       [0.3266717, 1.5968416, -0.8996342, 0.48808944,...         1378.402685  \n",
      "3       [-0.6976077, 1.2039477, -1.0641123, 1.9904385,...          316.431250  \n",
      "4       [-1.1109985, -0.6169655, -0.2853634, -0.853487...          536.154861  \n",
      "...                                                   ...                 ...  \n",
      "281658  [1.1619347, 1.252091, -1.1136101, 1.3099998, 1...         1832.638194  \n",
      "281659  [0.2114619, 1.06434, -2.4741518, 2.0127885, 1....          690.905382  \n",
      "281660  [-0.9250524, -2.2397268, 0.9461936, 0.47907868...         1040.718056  \n",
      "281661  [0.9123289, 2.2914681, -1.183999, -0.81985295,...         2033.015972  \n",
      "281662  [0.46674135, 0.97828007, -0.47262886, -0.42950...          816.725694  \n",
      "\n",
      "[281663 rows x 5 columns]\n",
      "       subject_id     prediction_time  boolean_value  \\\n",
      "0        10002221 2203-12-30 12:21:00          False   \n",
      "1        10002221 2204-06-27 16:57:00          False   \n",
      "2        10002221 2204-07-03 11:45:00          False   \n",
      "3        10002221 2204-07-03 22:34:00          False   \n",
      "4        10002221 2204-07-16 11:39:00          False   \n",
      "...           ...                 ...            ...   \n",
      "32531    19998330 2178-11-18 23:33:00          False   \n",
      "32532    19998330 2178-11-27 19:21:00          False   \n",
      "32533    19998330 2178-11-27 21:51:00          False   \n",
      "32534    19998330 2178-11-27 22:53:00          False   \n",
      "32535    19998330 2178-11-29 21:51:19          False   \n",
      "\n",
      "                                                features  time_to_event_days  \n",
      "0      [-0.068334974, 1.4898237, 0.29719907, 0.694096...         1082.997917  \n",
      "1      [-1.1571676, 1.0271944, -0.4795328, 1.7742524,...          902.806250  \n",
      "2      [-0.4494862, 0.92185396, -0.092470124, 1.50421...          897.022917  \n",
      "3      [-0.5977858, 0.6819075, -0.63687754, 1.9164785...          896.572222  \n",
      "4      [-0.57893336, 0.88729376, -0.23222159, 1.64921...          884.027083  \n",
      "...                                                  ...                 ...  \n",
      "32531  [-0.03145494, 0.07929867, 1.209682, 0.10075643...           20.018750  \n",
      "32532  [0.030787136, -0.61821353, 1.536746, 0.8457455...           11.193750  \n",
      "32533  [0.33604044, -0.5640281, 1.1253675, 0.967621, ...           11.089583  \n",
      "32534  [0.025627302, -0.9713896, 1.3276851, 0.7812983...           11.046528  \n",
      "32535  [-0.22304545, -0.47755194, 1.5952246, 0.399637...            9.089363  \n",
      "\n",
      "[32536 rows x 5 columns]\n",
      "        subject_id     prediction_time  boolean_value  \\\n",
      "0         14856789 2159-07-01 01:39:00          False   \n",
      "1         17714334 2154-05-11 16:30:00          False   \n",
      "2         18828728 2156-06-07 09:19:00          False   \n",
      "3         18613771 2135-06-16 07:15:00          False   \n",
      "4         11719852 2125-10-24 15:09:00          False   \n",
      "...            ...                 ...            ...   \n",
      "187516    16383947 2164-06-30 01:47:00          False   \n",
      "187517    15514336 2153-11-09 00:46:00          False   \n",
      "187518    17002650 2154-06-10 21:50:00          False   \n",
      "187519    17821868 2180-12-09 17:33:00          False   \n",
      "187520    16486571 2201-08-16 02:57:00          False   \n",
      "\n",
      "                                                 features  time_to_event_days  \n",
      "0       [-0.81256163, 0.09338329, -0.77497506, 1.73485...           60.931250  \n",
      "1       [-1.6759951, 1.7864869, 0.11399857, 1.0522957,...          436.744444  \n",
      "2       [-0.77201957, 1.2058021, -0.44299135, 0.636509...         3254.106250  \n",
      "3       [-1.3570073, 1.5016088, -0.023109365, 0.005771...         4169.697917  \n",
      "4       [0.060719736, -0.538253, -0.5489207, 1.6244895...           43.368750  \n",
      "...                                                   ...                 ...  \n",
      "187516  [0.48297068, 0.5652487, 0.8313463, -0.5837561,...         2755.395139  \n",
      "187517  [-0.5997072, 1.0708477, 1.1334537, 0.09449627,...         3236.968056  \n",
      "187518  [-0.5358748, -1.2601724, -1.0187278, 1.3216285...            2.881944  \n",
      "187519  [-1.3388371, 1.0393251, -0.12223328, 0.9877525...          778.035417  \n",
      "187520  [-0.8250077, 2.2638512, 0.32372096, -0.2750430...         3517.373611  \n",
      "\n",
      "[187521 rows x 5 columns]\n",
      "       subject_id     prediction_time  boolean_value  \\\n",
      "0        10002221 2204-06-27 16:57:00           True   \n",
      "1        10003637 2145-11-23 07:15:00          False   \n",
      "2        10003637 2146-01-22 23:08:00          False   \n",
      "3        10003637 2146-02-18 16:49:00          False   \n",
      "4        10003637 2149-05-13 15:41:00          False   \n",
      "...           ...                 ...            ...   \n",
      "21632    19998330 2178-09-20 20:20:00          False   \n",
      "21633    19998330 2178-10-01 07:28:00          False   \n",
      "21634    19998330 2178-10-10 15:23:00          False   \n",
      "21635    19998330 2178-10-21 15:14:00          False   \n",
      "21636    19998330 2178-11-27 21:51:00          False   \n",
      "\n",
      "                                                features  time_to_event_days  \n",
      "0      [-1.1571676, 1.0271944, -0.4795328, 1.7742524,...            2.830556  \n",
      "1      [0.8629135, 1.9000895, -0.7785396, 0.42618233,...         1641.656250  \n",
      "2      [0.98470867, 1.6819824, -1.3617259, 1.3521317,...         1580.994444  \n",
      "3      [1.612437, 1.649735, -1.422951, 1.4336216, 1.3...         1554.257639  \n",
      "4      [-0.4142071, 0.21552734, -0.7119818, 0.7775594...          374.304861  \n",
      "...                                                  ...                 ...  \n",
      "21632  [0.15210643, -0.77253664, 0.52066, 2.1636205, ...           79.152778  \n",
      "21633  [0.23874414, -1.1151481, 1.070131, 1.9494451, ...           68.688889  \n",
      "21634  [0.10699423, -0.951308, 0.86325145, 1.6662245,...           59.359028  \n",
      "21635  [0.06788901, -0.95876634, 1.4919431, 0.6424573...           48.365278  \n",
      "21636  [0.33604044, -0.5640281, 1.1253675, 0.967621, ...           11.089583  \n",
      "\n",
      "[21637 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# fetch tte labels from phenotype task to processed task\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import femr\n",
    "import femr.featurizers\n",
    "import femr.splits\n",
    "\n",
    "diseases = [\"stroke\",\"masld\"]\n",
    "# phenotype_dir = Path(\"/data/processed_datasets/processed_datasets/mimic/phenotype_task\")\n",
    "# file1 = pd.read_parquet('/data/processed_datasets/processed_datasets/mimic/phenotype_task/celiac/celiac_cohort.parquet')\n",
    "phenotype_dir = Path(\"/user/zj2398/cache/mimic/mimic-3.1-meds/phenotype_task\")\n",
    "embedding_dir = Path(\"/user/zj2398/cache/motor_mimic_8k/results\")\n",
    "for disease in diseases:\n",
    "    df1 = pd.read_parquet(phenotype_dir / disease / f\"{disease}_cohort.parquet\")\n",
    "    # print(cohort_label.head())\n",
    "\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        split_path = embedding_dir / disease/ f\"motor/features_with_label/{split}.parquet\"\n",
    "        with open(split_path, 'rb') as f:\n",
    "            df2 = pd.read_parquet(f)\n",
    "        print(df2)\n",
    "\n",
    "\n",
    "        # --- 0) (Optional) make copies so originals stay intact\n",
    "        df1_ = df1.copy()\n",
    "        df2_ = df2.copy()\n",
    "\n",
    "        # --- 1) Ensure dtypes match exactly\n",
    "        # prediction_time to datetime (naive)\n",
    "        df1_[\"prediction_time\"] = pd.to_datetime(df1_[\"prediction_time\"], errors=\"raise\", utc=False)\n",
    "        df2_[\"prediction_time\"] = pd.to_datetime(df2_[\"prediction_time\"], errors=\"raise\", utc=False)\n",
    "\n",
    "        # boolean_value to proper bool\n",
    "        # (handles cases where it's already bool or string \"True\"/\"False\")\n",
    "        df1_[\"boolean_value\"] = df1_[\"boolean_value\"].astype(str).str.lower().map({\"true\": True, \"false\": False})\n",
    "        df2_[\"boolean_value\"] = df2_[\"boolean_value\"].astype(str).str.lower().map({\"true\": True, \"false\": False})\n",
    "\n",
    "        # subject_id to consistent integer (or string if you prefer)\n",
    "        df1_[\"subject_id\"] = pd.to_numeric(df1_[\"subject_id\"], downcast=\"integer\")\n",
    "        df2_[\"subject_id\"] = pd.to_numeric(df2_[\"subject_id\"], downcast=\"integer\")\n",
    "\n",
    "        # --- 2) Check that the join key is unique on the right side (df1)\n",
    "        key_cols = [\"subject_id\", \"prediction_time\", \"boolean_value\"]\n",
    "        dupes = df1_.duplicated(subset=key_cols, keep=False)\n",
    "        if dupes.any():\n",
    "            # Show a few offending keys to help diagnose\n",
    "            raise ValueError(\n",
    "                \"df1 has duplicate rows for the join key; expected 1 row per key.\\n\"\n",
    "                f\"Examples:\\n{df1_.loc[dupes, key_cols].head(10)}\"\n",
    "            )\n",
    "\n",
    "        # --- 3) Merge (df2 left-join to df1), enforcing m:1 cardinality\n",
    "        merged = df2_.merge(\n",
    "            df1_[key_cols + [\"time_to_event_days\"]],\n",
    "            on=key_cols,\n",
    "            how=\"left\",\n",
    "            validate=\"m:1\"\n",
    "        )\n",
    "\n",
    "        # --- 4) Verify every df2 row matched something in df1\n",
    "        num_unmatched = merged[\"time_to_event_days\"].isna().sum()\n",
    "        if num_unmatched:\n",
    "            # Show a few missing keys to debug\n",
    "            missing_keys = merged.loc[merged[\"time_to_event_days\"].isna(), key_cols].drop_duplicates().head(10)\n",
    "            raise ValueError(\n",
    "                f\"{num_unmatched} row(s) in df2 did not find a match in df1. \"\n",
    "                f\"Examples of missing keys:\\n{missing_keys}\"\n",
    "            )\n",
    "\n",
    "        # --- 5) If you only want df2 + the added label:\n",
    "        df2_with_label = merged  # same columns as df2, plus time_to_event_days\n",
    "        # print(df2_with_label)\n",
    "        df2_with_label.to_parquet(split_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motor_chao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
